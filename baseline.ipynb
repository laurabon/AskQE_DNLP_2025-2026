{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# AskQE Pipeline - Qwen2.5-3B-Instruct Baseline\n",
                "\n",
                "This notebook runs the complete AskQE pipeline using the **Qwen/Qwen2.5-3B-Instruct** model.\n",
                "All results are saved in `results Qwen3B baseline/` folder.\n",
                "\n",
                "**Note:** Models are cached on Google Drive for faster subsequent runs."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "drive-header",
            "metadata": {},
            "source": [
                "## 0. Mount Google Drive & Configure Model Cache"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "drive-mount",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IN_COLAB:\n",
                "    from google.colab import drive\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    DRIVE_CACHE_DIR = '/content/drive/MyDrive/AskQE_Models_Cache'\n",
                "    os.makedirs(DRIVE_CACHE_DIR, exist_ok=True)\n",
                "    \n",
                "    os.environ['HF_HOME'] = DRIVE_CACHE_DIR\n",
                "    os.environ['TRANSFORMERS_CACHE'] = os.path.join(DRIVE_CACHE_DIR, 'transformers')\n",
                "    os.environ['SENTENCE_TRANSFORMERS_HOME'] = os.path.join(DRIVE_CACHE_DIR, 'sentence_transformers')\n",
                "    \n",
                "    print(f'Model cache directory: {DRIVE_CACHE_DIR}')\n",
                "else:\n",
                "    print('Not running in Colab - using default cache directories')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-header",
            "metadata": {},
            "source": [
                "## Setup - Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "\n",
                "subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'transformers', 'torch', 'accelerate', 'nltk', 'sentence-transformers', 'sacrebleu', 'textstat'], check=True)\n",
                "\n",
                "if IN_COLAB:\n",
                "    if not os.path.exists('/content/askqe'):\n",
                "        subprocess.run(['git', 'clone', 'https://github.com/Simone280802/AskQE_DNLP_2025-2026.git', '/content/askqe'], check=True)\n",
                "    PROJECT_ROOT = '/content/askqe'\n",
                "else:\n",
                "    PROJECT_ROOT = os.getcwd()\n",
                "\n",
                "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results Qwen3B baseline')\n",
                "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "\n",
                "print(f'Project root: {PROJECT_ROOT}')\n",
                "print(f'Results directory: {RESULTS_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "models-header",
            "metadata": {},
            "source": [
                "## Pre-download Models\n",
                "\n",
                "Download all models needed for the pipeline. These will be cached on Drive for reuse."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "models-download",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForCausalLM, LongformerTokenizer, LongformerForSequenceClassification\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import torch\n",
                "\n",
                "print('=== Downloading/Loading Models ===')\n",
                "print('This may take a while on first run, but will be cached on Drive for future use.\\n')\n",
                "\n",
                "MODELS = {\n",
                "    'qwen': 'Qwen/Qwen2.5-3B-Instruct',\n",
                "    'sbert': 'sentence-transformers/all-MiniLM-L6-v2',\n",
                "    'answerability': 'potsawee/longformer-large-4096-answerable-squad2'\n",
                "}\n",
                "\n",
                "# Download Qwen model\n",
                "print(f\"[1/3] Loading {MODELS['qwen']}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODELS['qwen'])\n",
                "model = AutoModelForCausalLM.from_pretrained(MODELS['qwen'], torch_dtype=torch.bfloat16, device_map='auto')\n",
                "del model, tokenizer\n",
                "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
                "print('      ✓ Qwen cached')\n",
                "\n",
                "# Download SBERT model\n",
                "print(f\"[2/3] Loading {MODELS['sbert']}...\")\n",
                "sbert_model = SentenceTransformer(MODELS['sbert'])\n",
                "del sbert_model\n",
                "print('      ✓ SBERT cached')\n",
                "\n",
                "# Download Answerability model (Longformer)\n",
                "print(f\"[3/3] Loading {MODELS['answerability']}...\")\n",
                "try:\n",
                "    ans_tokenizer = LongformerTokenizer.from_pretrained(MODELS['answerability'])\n",
                "    ans_model = LongformerForSequenceClassification.from_pretrained(MODELS['answerability'])\n",
                "    del ans_tokenizer, ans_model\n",
                "    print('      ✓ Answerability model cached')\n",
                "except Exception as e:\n",
                "    print(f'      ⚠ Could not load: {e}')\n",
                "\n",
                "print('\\n=== All models cached! ===')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qg-header",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Question Generation (QG)\n",
                "\n",
                "Generate questions for vanilla, atomic, and semantic pipelines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qg-vanilla",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'QG', 'code'))\n",
                "output_path = os.path.join(RESULTS_DIR, 'QG', 'vanilla_qwen-3b.jsonl')\n",
                "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "subprocess.run([sys.executable, '-u', 'qwen-3b.py', '--output_path', output_path, '--prompt', 'vanilla'], check=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qg-atomic",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_path = os.path.join(RESULTS_DIR, 'QG', 'atomic_qwen-3b.jsonl')\n",
                "subprocess.run([sys.executable, '-u', 'qwen-3b.py', '--output_path', output_path, '--prompt', 'atomic'], check=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qg-semantic",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_path = os.path.join(RESULTS_DIR, 'QG', 'semantic_qwen-3b.jsonl')\n",
                "subprocess.run([sys.executable, '-u', 'qwen-3b.py', '--output_path', output_path, '--prompt', 'semantic'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qa-header",
            "metadata": {},
            "source": [
                "## 2. Question Answering (QA)\n",
                "\n",
                "Run QA for ALL configurations automatically:\n",
                "- 3 pipelines (vanilla, atomic, semantic)\n",
                "- 5 languages (es, fr, hi, tl, zh)\n",
                "- 8 perturbations\n",
                "- Source and BT based"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qa-all",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'QA', 'code'))\n",
                "subprocess.run([sys.executable, '-u', 'qwen-3b.py', '--run_all'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "biomqm-header",
            "metadata": {},
            "source": [
                "## 3. BioMQM Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "biomqm-code",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'biomqm', 'askqe'))\n",
                "output_path = os.path.join(RESULTS_DIR, 'biomqm', 'askqe_qg_qwen3b.jsonl')\n",
                "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "subprocess.run([sys.executable, '-u', 'qwen-3b.py', '--output_path', output_path, '--prompt', 'atomic'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eval-header",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Evaluation Metrics\n",
                "\n",
                "### 4.1 SBERT"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eval-sbert",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'evaluation', 'sbert'))\n",
                "output_file = os.path.join(RESULTS_DIR, 'evaluation', 'sbert', 'qwen-3b.csv')\n",
                "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
                "subprocess.run([sys.executable, 'sbert.py', '--model', 'qwen-3b', '--output_file', output_file], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "string-header",
            "metadata": {},
            "source": [
                "### 4.2 String Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eval-string",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'evaluation', 'string-comparison'))\n",
                "subprocess.run([sys.executable, 'string_comparison.py'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qe-header",
            "metadata": {},
            "source": [
                "### 4.3 BT-Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qe-btscore",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'evaluation', 'bt-score'))\n",
                "subprocess.run([sys.executable, 'run_bt.py'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "desiderata-header",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Desiderata Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "des-all",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(PROJECT_ROOT, 'evaluation', 'desiderata'))\n",
                "subprocess.run([sys.executable, 'i_avg_questions.py'], check=True)\n",
                "subprocess.run([sys.executable, 'i_duplicate.py'], check=True)\n",
                "subprocess.run([sys.executable, 'i_diversity.py'], check=True)\n",
                "subprocess.run([sys.executable, 'q_answerability.py'], check=True)\n",
                "subprocess.run([sys.executable, 'q_readability.py'], check=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "complete-header",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Pipeline Complete!\n",
                "\n",
                "All results saved in `results Qwen3B baseline/`."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}